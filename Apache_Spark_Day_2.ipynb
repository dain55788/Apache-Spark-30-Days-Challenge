{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2: Setting up Apache Spark in Google Collab\n"
      ],
      "metadata": {
        "id": "412zRnwAYOVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, I started hands-on learning by **setting up PySpark** in Google Collab! Google Collab is a free cloud-based environment that makes it easy to experiment with PySpark without the hassle of installing complex setups on your personal computer"
      ],
      "metadata": {
        "id": "3pmzi_VPYUeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Choose Google Collab?\n",
        "### Google Collab allows you to run Python code in the cloud and provides access to GPU/TPU resources. Itâ€™s an ideal environment to test and run PySpark code directly in your browser without any complicated installation processes."
      ],
      "metadata": {
        "id": "AiwaLUwnYlQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Steps to Set Up PySpark on Jupyter**"
      ],
      "metadata": {
        "id": "0JDi2F9oY5ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1**: Open a new notebook in JupyterNotebook."
      ],
      "metadata": {
        "id": "N3eTMzFsakQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2**: Install PySpark and Java:\n",
        "\n",
        "Since Spark requires Java to run, we need to install both:"
      ],
      "metadata": {
        "id": "Mn11HnlQaqDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OXSJkCeXWhX",
        "outputId": "7c5281ae-9ced-4b33-ef69-7c24d4f8a21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation:\n",
        "\n",
        "\n",
        "*   apt-get installs OpenJDK 8 (an open-source version of Java).\n",
        "*   pip installs the PySpark library, ensuring all necessary components for PySpark are ready."
      ],
      "metadata": {
        "id": "1AChcgWJYGow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3**: Set the Java environment variable:"
      ],
      "metadata": {
        "id": "qw9_3VTQZ10R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "metadata": {
        "id": "Mh_qC9BTX0-J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation:\n",
        "Setting JAVA_HOME ensures PySpark can locate the correct Java path, avoiding version conflicts."
      ],
      "metadata": {
        "id": "5BF14szNaFu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4**: Create your first Spark Session:"
      ],
      "metadata": {
        "id": "G5-yMYkNaYqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Creat a Spark session\n",
        "spark = SparkSession.builder.appName(\"PySpark in Colab\").getOrCreate()\n",
        "# Print Spark Version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfkaQaXsX62S",
        "outputId": "fbfe3b4f-3826-49fd-ed8e-feee25085d30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ccvDyp2oYFOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš€\n",
        "## Tomorrow - Spark Architecture! On Day 3, weâ€™ll dive into Sparkâ€™s architecture, including Drivers, Executors, and Cluster Managers. Youâ€™ll understand how Spark manages data and executes tasks efficiently. Stay tuned!"
      ],
      "metadata": {
        "id": "r1rDli95bWOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h78YcdEfbY5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}