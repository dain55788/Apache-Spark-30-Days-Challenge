{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 30-Day Journey with Apache Spark⚡Day 6 🚩\n",
        "\n",
        "## **Day 6: Transformations and Actions in Spark** 🔄🎬\n",
        "Welcome to Day 6! Today, we’ll explore **two core operations in Spark**: **Transformations and Actions**. Understanding how these operations work will help you process data more efficiently in the Spark environment."
      ],
      "metadata": {
        "id": "TMojLXpeuGnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2cUJP9SSurGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1️⃣ Transformations**\n",
        "Definition:\n",
        "**Transformations** are operations applied to RDDs, DataFrames, or Datasets that return a new dataset but do not execute immediately. Transformations are lazy, meaning they are only executed when an Action is called.\n",
        "\n",
        "**Common Types of Transformations:**"
      ],
      "metadata": {
        "id": "FvWCGs8IusRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ **Map**: Applies a function to each element of the dataset and returns a new dataset.\n",
        "\n",
        "    `rdd.map(lambda x: x * 2)`\n",
        "\n",
        "+ **Filter**: Filters elements that satisfy a condition.\n",
        "\n",
        "    `rdd.filter(lambda x: x > 10)`\n",
        "\n",
        "+ **FlatMap**: Similar to map, but allows returning multiple elements from each input element.\n",
        "\n",
        "    `rdd.flatMap(lambda x: x.split(\" \"))`\n",
        "\n",
        "+ **Union**: Combines two datasets into one.\n",
        "\n",
        "    `rdd1.union(rdd2)`\n",
        "\n",
        "+ **Distinct**: Removes duplicate elements from the dataset.\n",
        "\n",
        "    `rdd.distinct()`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccFcLFAquQSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4rSdPUzwdIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2️⃣ Actions**\n",
        "\n",
        "Definition: **Actions** **trigger the execution of Transformations** and return the result to the driver program or save the data to external storage.\n",
        "\n",
        "**Common Types of Actions:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zw3l1HNNxS__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ **Collect**: Retrieves all elements of the dataset and returns them to the driver as a list.\n",
        "\n",
        "    `rdd.collect()`\n",
        "\n",
        "+ **Count**: Counts the number of elements in the dataset.\n",
        "\n",
        "    `rdd.count()`\n",
        "\n",
        "+ **Take**: Retrieves a specified number of elements from the dataset.\n",
        "\n",
        "    `rdd.take(5)`"
      ],
      "metadata": {
        "id": "-BqbmH0vyfU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vSUQsofqyhSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **𝗟𝗮𝘇𝘆 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻** ✅\n",
        "Spark uses lazy evaluation, meaning *Transformations are not immediately computed*. **Instead, Spark builds an Execution Plan as a DAG**(Directed Acyclic Graph).\n",
        "\n",
        "DAG optimization minimizes data redistribution (data shuffling) and improves performance.\n",
        "Execution occurs only when an Action is called, at which point the plan is executed."
      ],
      "metadata": {
        "id": "UpH9Ye8hx_23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔑 Key Takeaways:**\n",
        "\n",
        "#### **Transformations**: Lazy operations that build the execution plan.\n",
        "\n",
        "#### **Actions**: Trigger execution and return results or save data.\n",
        "\n",
        "#### **Lazy Evaluation**: Enables Spark to optimize the data processing workflow and use resources efficiently.\n",
        "\n",
        "#### Lazy Evaluation is a breakthrough in Spark, optimizing resource usage and minimizing unnecessary computations."
      ],
      "metadata": {
        "id": "y6ZPKHgfy3jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "0ETnL6zlzRuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 Practice Example: Getting started with **Transformation and Actions** in Apache Spark"
      ],
      "metadata": {
        "id": "V58cGoUhzeCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr9yExzetuOA",
        "outputId": "e396aaf3-7845-436d-aa6a-86c0bf0067aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Intalling Java JDK and Apache Spark:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "5Npq16nXztaq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create first Spark session:\n",
        "from pyspark.sql import SparkSession\n",
        "# Creat a Spark session\n",
        "spark = SparkSession.builder.appName(\"PySpark in Colab\").getOrCreate()\n",
        "# Print Spark Version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apMNomFWz_8M",
        "outputId": "4e1d9fa4-952f-4e82-9070-cd613f4f19d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations"
      ],
      "metadata": {
        "id": "mjHdbMKp0Xq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 4, 5])\n",
        "mapped_rdd = rdd.map(lambda x: x*2)\n",
        "filtered_rdd = rdd.filter(lambda x: x > 2)\n",
        "distinct_rdd = rdd.distinct()\n",
        "print(mapped_rdd.collect())\n",
        "print(filtered_rdd.collect())\n",
        "print(distinct_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se_h7YW-0B-y",
        "outputId": "884c4797-786f-4c3e-f2bc-0d6ed0bb95a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 8, 10]\n",
            "[3, 4, 4, 5]\n",
            "[2, 4, 1, 3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions"
      ],
      "metadata": {
        "id": "pNUNRWQe1f2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "total_count = rdd.count()\n",
        "first_element = rdd.first()\n",
        "sum_elements = rdd.sum() #or using sum_elements = rdd.reduce(lambda a, b: a + b)\n",
        "print(f'{total_count} {first_element} {sum_elements}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZhgzUfG08kK",
        "outputId": "c4e3dcd4-0464-4edc-8599-11c4a5f04eae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 1 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "997wGbQ02Cnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}